{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': torch.Size([1115394]),\n",
       " 'train': torch.Size([1003854]),\n",
       " 'validation': torch.Size([111540]),\n",
       " 'sample': tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "         53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "          1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "         57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "          6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "         58, 47, 64, 43, 52, 10,  0, 37, 53, 59])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/tinyshakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "# encoder: take a string, output a list of integers\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "\n",
    "# decoder: take a list of integers, output a string\n",
    "def decode(l):\n",
    "    return \"\".join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9 * data.nelement())\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "{\n",
    "    \"data\": data.shape,\n",
    "    \"train\": train_data.shape,\n",
    "    \"validation\": val_data.shape,\n",
    "    \"sample\": data[:100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # How many independent sequences will we process in parallel?\n",
    "block_size = 8  # What is the maximum context length for predictions?\n",
    "n_embed = 32  # Size of embeddings\n",
    "max_steps = 10_000  # Learning\n",
    "lr = 1e-3  # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.self_attention_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)  # Language modelling head\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # Why is `+` the preferred choice here?\n",
    "        x = self.self_attention_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:  # Targets will be none during inference\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape  # Batch, Time, Embedding\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "        self.query = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "        self.value = nn.Linear(in_features=n_embed, out_features=head_size, bias=False)\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)  # (B, T, C)\n",
    "        q = self.query(x)  # (B, T, C)\n",
    "\n",
    "        # Compute attention scores (affinities)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, C) @ (B, C, T) â†’ (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "\n",
    "        # Weighted aggregation of values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) => (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 65])\n",
      "tensor(4.2068, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "ee'nCXLFs?Z,gfrx.dJ$tRFCEun U3pmPej3Pt-y?'SwrWl!$K-:I,IwIR$&,?cNIUN'wONtErKcfjmKZvF,GvpzGweVoHSrQJs,\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "m = BigramLanguageModel()\n",
    "\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# fmt: off\n",
    "print(\n",
    "    decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(m.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start=4.199311256408691\n",
      "end=2.30338978767395\n"
     ]
    }
   ],
   "source": [
    "for step in range(max_steps):\n",
    "    # Get a minibatch\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step == 0:\n",
    "        print(f\"start={loss.item()}\")\n",
    "\n",
    "print(f\"end={loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turl-chee\n",
      "'A Pout;\n",
      "I myoum, yousshou tand hererord oche ty Xas faur ciuprue:\n",
      "Hot as thaillgout mandd dist ye fay mis hary ncik nance hadis ogoengwnge h'doursorno me lallod branlle\n",
      "lat lyof mak Wiflld, our nd,\n",
      "CHor is hat sen ow tteveccitel spe sthes.\n",
      "\n",
      "LORD:\n",
      "WHAe.\n",
      "\n",
      "There she dakn grefre sour tat apry, ke my. LACit imind\n",
      "Ma nisad! yus ngaruapre,\n",
      "'Thave mou? Thy Lot\n",
      "Fiofr tre mpm terar ckn;\n",
      "Aay hack \n"
     ]
    }
   ],
   "source": [
    "tokens = m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=400)\n",
    "\n",
    "print(decode(tokens[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
